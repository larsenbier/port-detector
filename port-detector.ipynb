{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12747372,"sourceType":"datasetVersion","datasetId":8058305},{"sourceId":258751239,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ***************************************************************************\n# --------------------------------- Imports ---------------------------------\n# ***************************************************************************\n\nimport numpy as np # linear algebra\n\nimport os # file handling\nimport json # file handling\nimport pickle # file handling\nimport zipfile # file handling\n\nimport torch # deep learning\nimport torch.nn as nn\nimport torchvision # deep learning for computer vision\nfrom torch.utils.data import Dataset # shortcuts for writing dataset\n\nimport tqdm # progress bar\n\nimport matplotlib.pyplot as plt # graphing\n\nimport PIL\n\nimport random # data loader\n\nimport time # timing\n\nimport cv2 # object detection","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:42:08.252005Z","iopub.execute_input":"2025-09-04T15:42:08.252324Z","iopub.status.idle":"2025-09-04T15:42:08.258384Z","shell.execute_reply.started":"2025-09-04T15:42:08.252301Z","shell.execute_reply":"2025-09-04T15:42:08.257205Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# ***************************************************************************\n# ------------------------------- Dataset Info ------------------------------\n# ***************************************************************************\n# 1) Classes are:\n    # 0-background\n    # 1-connected\n    # 2-empty","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:20:00.627340Z","iopub.execute_input":"2025-09-04T15:20:00.628294Z","iopub.status.idle":"2025-09-04T15:20:00.632535Z","shell.execute_reply.started":"2025-09-04T15:20:00.628258Z","shell.execute_reply":"2025-09-04T15:20:00.631510Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# ***************************************************************************\n# --------------------- Object Detection Dataset Class ----------------------\n# ***************************************************************************\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass PortDataset(Dataset):\n    def __init__(self,root,pkl_images,pkl_targets):\n        '''\n        dataset for Port (object detection version).\n\n        arguments\n            root: the root path of the folder where the images live\n            pkl_images: the path of the pickled (list) version of the image filenames\n            pkl_targets: the path of the pickled (list) version of the image annotations\n\n        Note that roboflow did all the transforming before we downloaded the data. If we need more transformations, we can go back and download the unedited version, then implement our own transformations.\n        '''\n        self.root=root\n        self.filenames=pkl_images\n        self.targets=pkl_targets\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self,idx):\n        if type(idx) is not int:\n            raise ValueError(f'expected idx to be an integer, got {type(idx)}')\n        # Image tensor\n        image=torchvision.io.read_image(os.path.join(self.root,self.filenames[idx])).to(torch.float32)\n        # Targets tensor\n        boxes=torch.tensor(self.targets[idx]['boxes'])\n        labels=torch.tensor(self.targets[idx]['labels'])\n        targets={'boxes':boxes,'labels':labels}      \n        \n        return image,targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:20:01.405285Z","iopub.execute_input":"2025-09-04T15:20:01.405583Z","iopub.status.idle":"2025-09-04T15:20:01.413733Z","shell.execute_reply.started":"2025-09-04T15:20:01.405563Z","shell.execute_reply":"2025-09-04T15:20:01.412622Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# ***************************************************************************\n# --------------------- Generate file lists for dataset ---------------------\n# ***************************************************************************\n\ndef pickle_data(subset,root):\n    '''\n    Turns the data from port-by-ds-on-robolab into lists of filepaths, boxes, and labels which can be used by pytorch.\n    Saves the output to the kaggle working directory.\n    '''\n    if subset not in ['test','train','valid']:\n        raise ValueError(f'please enter the string \"test\", \"valid\", or \"train\". Received {subset}')\n    \n    # Read in COCO json data\n    fpath=f'{root}/{subset}/_annotations.coco.json'\n    with open(fpath,encoding='utf-8') as f:\n        data=json.load(f)\n        \n    # Make images: a list of the image pathways\n    images=[None]*len(data['images'])\n    \n    # Populate images\n    for image in data['images']:\n        idx=image['id']\n        images[idx]=os.path.join(root,subset,image['file_name'])\n        \n    # Make targets: a list of distinct dictionaries\n    targets=[None]*len(data['images'])\n    for i in range(len(targets)):\n        targets[i]={'boxes':[],'labels':[]}\n    \n    # Populate targets\n    for note in data['annotations']:\n        # Get image index\n        image_idx=note['image_id']\n        # change bounding box representation from (x,y,w,h) (upper-left and size) to (x1,y1,x2,y2) (upper-left and bottom-right)\n        x,y,w,h=note['bbox'].copy()\n        bbox=[x,y,x+w,y+h]\n        if bbox[0]>=bbox[2] or bbox[1]>=bbox[3]:\n            raise Exception(f'expected x1,y1 to be less than x2,y2 respectively. Got box {bbox}')\n        # Add box to d\n        targets[image_idx]['boxes'].append(bbox)\n        # Add label to d\n        targets[image_idx]['labels'].append(note['category_id'])\n    \n    # Check that boxes and labels are in bijective correspondence\n    total_boxes=0\n    for i in range(0,len(targets)):\n        assert(len(targets[i]['boxes'])==len(targets[i]['labels']))\n        total_boxes+=len(targets[i]['boxes'])\n    \n    # Remove images with no labeled boxes\n    i=0\n    while i<len(images):\n        if len(targets[i]['boxes'])==0:\n            targets.pop(i)\n            images.pop(i)\n        else:\n            i+=1\n    \n    # Save data\n    with open(f'/kaggle/working/{subset}_images.pkl','wb') as f:\n        pickle.dump(images, f)\n    with open(f'/kaggle/working/{subset}_targets.pkl','wb') as f:\n        pickle.dump(targets, f)\n\n\n\npickle_data('test','/kaggle/input/port-by-ds-on-robolab')\ntest_dataset=PortDataset('/kaggle/input/port-by-ds-on-robolab/test',\n                  pickle.load(open('/kaggle/working/test_images.pkl','rb')),\n                  pickle.load(open('/kaggle/working/test_targets.pkl','rb'))\n                 )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:21:15.839272Z","iopub.execute_input":"2025-09-04T15:21:15.840082Z","iopub.status.idle":"2025-09-04T15:21:15.870188Z","shell.execute_reply.started":"2025-09-04T15:21:15.840055Z","shell.execute_reply":"2025-09-04T15:21:15.869432Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# ***************************************************************************\n# ------------------------- ResNet18 Implementation -------------------------\n# ***************************************************************************\n\ndef conv3x3(in_planes:int,out_planes:int,stride:int=1,groups:int=1,dilation:int=1)->nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        groups=groups,\n        bias=False,\n        dilation=dilation,\n    )\n\n\ndef conv1x1(in_planes:int,out_planes:int,stride:int=1)->nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes,out_planes,kernel_size=1,stride=stride,bias=False)\n    \n\nclass BottleneckBlock(nn.Module): # size halves iff we multiply channels by 2 (architecture decision, not code enforced)\n    def __init__(self,in_channels,out_channels,stride):\n        super(BottleneckBlock,self).__init__()\n        self.conv1=conv1x1(in_channels,in_channels)\n        self.bn1=nn.BatchNorm2d(in_channels) \n        self.conv2=conv3x3(in_channels,out_channels,stride) # Use the Bottleneck approach (downsample input on 3x3 conv)\n        self.bn2=nn.BatchNorm2d(out_channels)\n        self.conv3=conv1x1(out_channels,out_channels)\n        self.bn3=nn.BatchNorm2d(out_channels)\n        self.relu=nn.ReLU(inplace=True)\n        self.stride=stride\n        self.identity=lambda x: x\n        # reshape residual connection to match outputs (need to downsamples)\n        if stride!=1 or in_channels!=out_channels:\n             self.identity = nn.Sequential(\n                conv1x1(in_channels,out_channels,stride),\n                nn.BatchNorm2d(out_channels)\n            )\n\n\n    def forward(self,x:torch.Tensor)->torch.Tensor:\n        out=self.conv1(x)\n        out=self.bn1(out)\n        out=self.relu(out)\n        \n        out=self.conv2(out)\n        out=self.bn2(out)\n        out=self.relu(out)\n        \n        out=self.conv3(out)\n        out=self.bn3(out)\n        out+=self.identity(x)\n        out=self.relu(out)\n\n        return out\n\n\nclass ResNet18(nn.Module):\n    def __init__(self,num_classes:int):\n        \"\"\"\n        Creates a ResNet18 module with num_classes classes.\n        \"\"\"\n        super(ResNet18,self).__init__()\n        self.num_classes=num_classes\n        self.in_channels=64 # update the in_channels for the next layer after we make a layer\n        \n        self.conv1=nn.Conv2d(in_channels=3,out_channels=self.in_channels,kernel_size=(7,7),stride=2)\n        self.bn1=nn.BatchNorm2d(self.in_channels)\n        self.relu=nn.ReLU(inplace=True)\n        self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n\n        self.layer1=self._make_layer(out_channels=self.in_channels,stride=1) #in_channels=64\n        self.layer2=self._make_layer(out_channels=self.in_channels*2,stride=2) #in_channels=64\n        self.layer3=self._make_layer(out_channels=self.in_channels*2,stride=2) #in_channels=128\n        self.layer4=self._make_layer(out_channels=self.in_channels*2,stride=2) #in_channels=256\n\n        self.avgpool=nn.AdaptiveAvgPool2d(output_size=(1,1))\n        self.fc=nn.Linear(in_features=self.in_channels,out_features=self.num_classes)\n\n        self.softmax=nn.Softmax(dim=1)\n\n        # initialize weights using Kaiming initialization\n        self.apply(self._init_weights)\n\n    \n    def forward(self,x:torch.Tensor)->torch.Tensor:\n        x=self.conv1(x)\n        x=self.bn1(x)\n        x=self.relu(x)\n        x=self.maxpool(x)\n        \n        x=self.layer1(x)\n        x=self.layer2(x)\n        x=self.layer3(x)\n        x=self.layer4(x)\n\n        x=self.avgpool(x)\n        x=torch.flatten(x,1)\n        x=self.fc(x)\n\n        return x\n\n\n    def predict(self,x):\n        probs=self.softmax(self.forward(x))\n        return torch.argmax(probs,dim=1)\n        \n\n    def _make_layer(self,out_channels:int,stride:int)->nn.Sequential:\n        \"\"\"\n        Makes a block layer.\n        The first block has stride 1 to preserve the dimension of the input, and the second block has stride \"stride\" to achieve the output dimension.\n        \"\"\"\n        layer=nn.Sequential(\n            BottleneckBlock(self.in_channels,out_channels,1),\n            BottleneckBlock(out_channels,out_channels,stride)\n        )\n        self.in_channels=out_channels\n        return layer\n\n\n    def _init_weights(self,m):\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n            # Kaiming initialization (good for ReLU-based nets)\n            nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n            if m.bias is not None:\n                nn.init.constant_(m.bias,0.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:03:05.391651Z","iopub.execute_input":"2025-09-04T15:03:05.392045Z","iopub.status.idle":"2025-09-04T15:03:05.411031Z","shell.execute_reply.started":"2025-09-04T15:03:05.392014Z","shell.execute_reply":"2025-09-04T15:03:05.409368Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ***************************************************************************\n# --------------------- Sliding Window Object Detector ----------------------\n# ***************************************************************************\n\ndef sliding_window(image,step,ws):\n    \"\"\"\n    Generator that yields a sub-image in a sliding-window iteration scheme.\n\n    Args:\n        -image (torch.Tensor): the image to slide the window across.\n        -step (int): the number of pixels to step when sliding (both horizontally and vertically when reaching the end of a row).\n        -ws (tuple[int]): window size (H,W).\n    \"\"\"\n    # slide a window across the image\n    for y in range(0,image.shape[1]-ws[1],step):\n        for x in range(0, image.shape[2]-ws[0],step):\n            # yield the current window\n            yield (x,y,image[:,y:y+ws[1],x:x+ws[0]])\n\n\ndef image_pyramid(image,scale=1.5,minSize=(224, 224)):\n    \"\"\"\n    Generator that yields images progressively scaled by a factor of \"scale\" until reaching a size below \"min_size\".\n\n    Args:\n        -image (torch.Tensor): the image to construct the image pyramid from. The shape is (C,H,W).\n        -scale: (float): the factor by which the image is scaled down each iteration.\n        -min_size (tuple[int]): the dimensions of the smallest permitted image (the \"top of the pyramid\"). The entries are (max_rows,max_cols).\n    \"\"\"\n    # yield the original image\n    yield image\n    # keep looping over the image pyramid\n    while True:\n        # compute the dimensions of the next image in the pyramid\n        w=int(image.shape[2]/scale)\n        h=int(image.shape[1]/scale)\n\n        resizer=torchvision.transforms.Resize((h,w)) # images are C,H,W\n        \n        image=resizer(image)\n        # if the resized image does not meet the supplied minimum\n        # size, then stop constructing the pyramid\n        if image.shape[1]<minSize[0] or image.shape[2]<minSize[1]:\n            break\n        # yield the next image in the pyramid\n        yield image\n\n\ndef alt_NMS(boxes:list[tuple],proba:list[float],IoU_thresh:float):\n    \"\"\"\n    An alternate version of non-maximum suppression that sorts the boxes by their lower right y coordinate.\n    \"\"\"\n    # lists to store picked indices\n    picks=[]\n    # convert the list of boxes to an array for slicing\n    boxes_pick=np.array(boxes)\n    proba_pick=np.array(proba)\n    # get box coordinates (as arrays)\n    x1=boxes_pick[:,0]\n    y1=boxes_pick[:,1]\n    x2=boxes_pick[:,2]\n    y2=boxes_pick[:,3]\n    # compute area of each box and sort by bottom right y coordinate\n    area=(x2-x1+1)*(y2-y1+1)\n    idxs=np.argsort(y2)\n    # perform nms\n    while len(idxs)>0:\n        # pick the last index\n        last=len(idxs)-1\n        i=idxs[last]\n        picks.append(i)\n        suppress=[last]\n        # loop over all other boxes and check the overlap\n        for pos in range(last):\n            j=idxs[pos]\n            # find the overlapping region of the boxes\n            xx1=max(x1[i],x1[j])\n            yy1=max(y1[i],y1[j])\n            xx2=min(x2[i],x2[j])\n            yy2=min(y2[i],y2[j])\n            # compute width and height of overlapping region\n            w=max(0,xx2-xx1+1)\n            h=max(0,yy2-yy1+1)\n            # compute overlap\n            overlap=float(w*h)/area[j]\n            # eliminate boxes with too much overlap\n            if overlap>IoU_thresh:\n                suppress.append(pos)\n\n        # delete suppressed boxes\n        idxs=np.delete(idxs,suppress)\n\n    return boxes_pick[picks],proba_pick[picks]\n    \n\ndef non_maximum_suppression(boxes:list[tuple],probs:list[float],IoU_thresh:float):\n    \"\"\"\n    Runs non maximum suppression on a single class after sorting the boxes by probability.\n\n    Args:\n        -boxes (list[tuple]): a list of the bounding boxes detected (all for the same class and image).\n        -probs (list[float]): a list of the confidences (probabilities) of each box.\n        -IoU_thresh (float) the minumum IoU to supress a box.\n\n    Returns:\n        -list[tuple]: a list of the NMS boxes.\n        -list[float]: the probabilities of the NMS boxes.\n    \"\"\"\n    # lists of picked poxes and their probabilities\n    picks=[]\n    prob_picks=[]\n    # sort boxes and probs by probabilities (small-->large).\n    sorted_idx=np.argsort(probs)\n    sorted_boxes=[boxes[i] for i in sorted_idx]\n    sorted_probs=[probs[i] for i in sorted_idx]\n\n    while len(sorted_boxes)!=0:\n        # set the \"last\" (most likely) box\n        last=sorted_boxes[-1]\n        picks.append(last)\n        prob_picks.append(sorted_probs[-1])\n        sorted_boxes.pop(-1)\n        sorted_probs.pop(-1)\n\n        # check IoU for all other boxes\n        for i,box in enumerate(sorted_boxes):\n            # compute IoU\n            x1,y1,x2,y2=last\n            x3,y3,x4,y4=box\n            if x1<=x4 and y1<=y4 and x2>=x3 and y2>=y3: # overlapping boxes\n                I=(min(x2,x4)-max(x1,x3))*(min(y2,y4)-max(y1,y3))\n                A1=(x2-x1)*(y2-y1)\n                A2=(x4-x3)*(y4-y3)\n                U=A1+A2-I\n                if U==0:\n                    raise ZeroDivisionError(f'divide by zero encountered at box {box} compared against pick {last}; iter {i} of inner loop. I={I},U={U}.')\n                IoU=I/U\n                # suppress boxes with lots of overlap\n                if IoU>IoU_thresh:\n                    sorted_boxes.pop(i) \n                    sorted_probs.pop(i)\n    \n    return picks,prob_picks\n\n\ndef detect_ports(model:torch.nn.Module,orig_image:torch.Tensor,pyr_scale:float=1.5,min_pyr_size:tuple[int]=(300,300),win_step:int=24,roi_size:tuple[int]=(64,64),input_size:tuple[int]=(64,64),min_confidence:float=0.95,verbose=False):\n    # original image dimensions (used for getting coordinates of sliding windows)\n    orig_height,orig_width=orig_image.shape[:2]\n    # lists to store the ROIs (images) and the coordaintes\n    rois=[]\n    locs=[]\n    # runtime tracker\n    start=time.time()\n    # extract ROI s and coordinates\n    pyramid=image_pyramid(orig_image,scale=pyr_scale,minSize=min_pyr_size)\n    for image in pyramid:\n        resizer=torchvision.transforms.Resize(input_size)\n        scale=orig_width/float(image.shape[1])\n        # run sliding window\n        for (x,y,roiOrig) in sliding_window(image,win_step,roi_size):\n            x=int(x*scale)\n            y=int(y*scale)\n            w=int(roi_size[0]*scale)\n            h=int(roi_size[1]*scale)\n            roi=resizer(roiOrig)\n            rois.append(roi)\n            locs.append((x,y,x+w,y+h))\n    # runtime tracker\n    end=time.time()\n    if verbose:\n        print(f'[INFO] looping over pyramid/windows took {end-start:.5f} seconds')\n    \n    if verbose:\n        print('[INFO] classifying ROIs...')\n    # aggregate ROIs into one tensor\n    inputs=torch.stack(rois,dim=0).to(device)\n    # track runtime\n    start=time.time()\n    # get predictions for all ROIs\n    logits=torch.nn.Softmax(dim=1)(model(inputs))\n    # track runtime\n    end=time.time()\n    if verbose:\n        print(f'[INFO] classifying ROIs took {end-start:.5f} seconds')\n    del inputs\n\n    # organize predicted boxes by label\n    labels = {}\n    for i,logit in enumerate(logits):\n        label=torch.argmax(logit).item()\n        prob=logit[label].item()\n        if prob>=min_confidence:\n            box=locs[i]\n            L=labels.get(label,[])\n            L.append((box,prob))\n            labels[label]=L\n            \n    label_names=['background','connected','empty']\n    # run non maximum suppression\n    for label in labels.keys():\n        # visualization before NMS\n        if verbose:\n            print(f'[INFO] showing results for \"{label_names[label]}\"')\n            clone=orig_image.detach().clone().permute(1,2,0).numpy()\n            for box,prob in labels[label]:\n                (startX,startY,endX,endY)=box\n                cv2.rectangle(clone,(startX,startY),(endX, endY),(0, 255, 0),2)\n            clone=clone.astype(np.uint8)\n            plt.imshow(clone)\n            plt.title(f'Before NMS ({label_names[label]})')\n            plt.show()\n        \n        # run NMS\n        boxes=np.array([p[0] for p in labels[label]])\n        proba=np.array([p[1] for p in labels[label]])\n        boxes,proba=non_maximum_suppression(boxes,proba,IoU_thresh=0.8)\n        # restructure labels to be the same structure as before\n        labels[label]=[(boxes[i],proba[i]) for i in range(len(boxes))]\n\n        # visualization after NMS\n        if verbose:\n            clone=orig_image.detach().clone().permute(1,2,0).numpy()\n            for startX,startY,endX,endY in [p[0] for p in labels[label]]:\n                cv2.rectangle(clone,(startX,startY),(endX, endY),(0, 255, 0),2)\n            clone=clone.astype(np.uint8)\n            plt.imshow(clone)\n            plt.title(f'After NMS ({label_names[label]})')\n            plt.show()\n\n    return labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:25:26.644635Z","iopub.execute_input":"2025-09-04T15:25:26.644940Z","iopub.status.idle":"2025-09-04T15:25:26.660059Z","shell.execute_reply.started":"2025-09-04T15:25:26.644919Z","shell.execute_reply":"2025-09-04T15:25:26.659059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ***************************************************************************\n# ----------------------------- Load Best Model -----------------------------\n# ***************************************************************************\n\ndevice='cpu'\n# initialize ResNet18 with 3 output classes (background, empty, connected)\nmodel=ResNet18(3).to(device)\n# load best weights\nstate_dict=torch.load('/kaggle/input/port-classifier/best_weights.pth',map_location=device)\nmodel.load_state_dict(state_dict)\n# set to eval mode and display architecture\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:37:40.019543Z","iopub.execute_input":"2025-09-04T15:37:40.019901Z","iopub.status.idle":"2025-09-04T15:37:40.191777Z","shell.execute_reply.started":"2025-09-04T15:37:40.019880Z","shell.execute_reply":"2025-09-04T15:37:40.190648Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"ResNet18(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2))\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BottleneckBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (1): BottleneckBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BottleneckBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (identity): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BottleneckBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (identity): Sequential(\n        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (layer3): Sequential(\n    (0): BottleneckBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (identity): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BottleneckBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (identity): Sequential(\n        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (layer4): Sequential(\n    (0): BottleneckBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (identity): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BottleneckBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (identity): Sequential(\n        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=3, bias=True)\n  (softmax): Softmax(dim=1)\n)"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"# ***************************************************************************\n# ------------------------ Run Test on Sample Image -------------------------\n# ***************************************************************************\n# The testing stage never progressed beyond manually inspecting outputs from the object detector.\n# This is because the results were obviously inadequate, so quantitative testing was not necessary.\n\n# index of test image\nidx=17\n# test image\nimage=test_dataset[idx][0]\n# run object detection. Display outputs by setting \"verbose\" arg to True\nlabels=detect_ports(model,image,min_pyr_size=(200,200),win_step=16,roi_size=(64,64),min_confidence=0.8,verbose=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:41:06.585340Z","iopub.execute_input":"2025-09-04T15:41:06.586227Z","iopub.status.idle":"2025-09-04T15:41:30.468539Z","shell.execute_reply.started":"2025-09-04T15:41:06.586187Z","shell.execute_reply":"2025-09-04T15:41:30.467736Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}